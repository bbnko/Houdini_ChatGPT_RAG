{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZWLKCxgxkUNYna8qnTe85"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "53pGJEUqkOTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c8O0F2DkJNs",
        "outputId": "a501433c-4846-4254-a041-8a844ab15408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install libraries"
      ],
      "metadata": {
        "id": "3fHgwvEIkWtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv langchain langchain-openai openai langchain-community langchain_objectbox langchainhub gradio"
      ],
      "metadata": {
        "id": "-yVW8G0MkWL4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "YGI6o3JbwXLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.4\n",
        "top_p = 0.8"
      ],
      "metadata": {
        "id": "iiZibYiBwW8G"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Api key"
      ],
      "metadata": {
        "id": "y2ZCbcl_kU_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"drive/MyDrive/Colab/.env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2liGpGdkdNV",
        "outputId": "44e24583-cb8f-4db2-976e-6637517a1a68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "cI0wUJaAp8r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "txt_file_path = 'drive/MyDrive/Colab/docs/houdini_dataset.txt'\n",
        "loader = TextLoader(file_path=txt_file_path, encoding=\"utf-8\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "6zz0WGZCp8f4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "TaYI9ORgqkqx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "GziCir4prH2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_objectbox.vectorstores import ObjectBox\n",
        "vector = ObjectBox.from_documents(documents, OpenAIEmbeddings(), embedding_dimensions=768)"
      ],
      "metadata": {
        "id": "-SIq0lvHqhrL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline"
      ],
      "metadata": {
        "id": "J4iNwKI4sJT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "8Yx3k8Xqr7Mi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm = ChatOpenAI(model=selected_model)\n",
        "llm = ChatOpenAI(model=selected_model, temperature=temperature, model_kwargs={'top_p': top_p})\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "YtUDMMMDsMSY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overwrite prompt template"
      ],
      "metadata": {
        "id": "wz87MiKWvEfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.messages[0].prompt.template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Firstly provide a short summarized answer. Afterwards provide a detailed answer.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\""
      ],
      "metadata": {
        "id": "0sjZsfSgvDan"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA chain"
      ],
      "metadata": {
        "id": "_nhnFJaovSPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=vector.as_retriever(),\n",
        "        chain_type_kwargs={\"prompt\": prompt}\n",
        "    )"
      ],
      "metadata": {
        "id": "tMPg7MqevUKm"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How to create velocity field from geometry in houdini?\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8tnEF6VwFBc",
        "outputId": "f5f6c3c3-1eec-4ce6-8886-7085b95b54c7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How to create velocity field from geometry in houdini?',\n",
              " 'result': \"To create a velocity field from geometry in Houdini, you can use the 'vdb from polygons' node to efficiently convert meshes into sdf or fog, and create a velocity field if you have @v on your mesh. If you don't have a watertight mesh, you may need to create manifold geometry with extrudes. Another method involves using a volume wrangle to create a generic fog of density and then do an attribute transfer of @v from the curve to @vel of the volume.\\n\\nDetailed Answer:\\nTo create a velocity field from geometry in Houdini, you can follow these steps:\\n\\n1. Use the 'vdb from polygons' node to convert meshes into sdf or fog. This node can also create a velocity field if your mesh has @v attribute.\\n2. If you have a mesh like a pig, give it point normals and create a swirled @v in a wrangle with @v = cross(@N,{0,1,0}).\\n3. Use the 'vdb from polygons' node to make a density field and create a @vel field from @v.\\n4. If you don't have a watertight mesh, you may need to create manifold geometry with extrudes to avoid confusion in the vdb convert sop.\\n5. Another method involves using a volume wrangle. In this case, you can use a volume wrangle to create a generic box of density and then use a volume wrangle to find the nearest point on a curve and transfer its @v to @vel of the volume.\\n6. Make sure to wire the volume wrangle into the volume trail to visualize the result.\\n\\nAdditionally, you can use the 'vdb from particles' sop to convert particles to a volume field and then use that volume as a source for a pyro/smoke sim. This approach can give smoother results and more control over the simulation. Just be aware that using a large particle size with 'vdb from particles' may create inner and outer 'walls' of velocity where the particles conform to the shape, leaving an empty groove in the middle. Adjusting the particle radius or ensuring chaotic particle movement can help avoid this issue.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding UI"
      ],
      "metadata": {
        "id": "9aUVHfeGxM4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Initialize a global list to keep track of conversation history\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "def talk(message, history):\n",
        "    # The 'history' parameter includes past interactions\n",
        "    # We'll extract the question and answer texts to build a full context\n",
        "    context = \" \".join([f\"Q: {exchange['input']} A: {exchange['output']}\" for exchange in history])\n",
        "    full_question = f\"{context} Q: {message}\"\n",
        "\n",
        "    # Assume 'qa_chain' is your model that processes the full question\n",
        "    result = qa_chain.invoke({\"query\": full_question})\n",
        "    answer = result['result']\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Define the chatbot interface layout and properties\n",
        "TITLE = \"Houdini Chatbot\"\n",
        "DESCRIPTION = f\"{selected_model} with RAG to answer Houdini-related questions based on tokeru.com/cgwiki\"\n",
        "chatbot = gr.Chatbot(\n",
        "    show_label=True,\n",
        "    show_share_button=True,\n",
        "    show_copy_button=True,\n",
        "    likeable=True,\n",
        "    layout=\"bubble\",\n",
        "    bubble_full_width=True,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "# Create the ChatInterface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=talk,\n",
        "    chatbot=chatbot,\n",
        "    theme=\"Soft\",\n",
        "    title=TITLE,\n",
        "    description=DESCRIPTION\n",
        ")"
      ],
      "metadata": {
        "id": "iX3lVE5TyQWs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "NmyspIqDw77O",
        "outputId": "d9d511ca-ad2e-464e-a9e9-983a3bf96eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://007958e15839f17257.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://007958e15839f17257.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}